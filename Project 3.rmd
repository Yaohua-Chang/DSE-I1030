---
title: 'Project 3 Sea Surface  Temperature Measurements'
runtime: html_document
output:
  html_document: default
  pdf_document: default
---
<div id="instructions">
Due at 11:59 pm on Thursday, October 25<br />
Name: Yaohua Chang  
Name: Jiffar Abakoyas
</div>

TODO:Link other datsets to one
TODO:Fill out table and conclusion
TODO:Compelte Problem 6 & 7
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, message=FALSE, warning=FALSE}
#Initial Libraries
library(ncdf4)
library(leaps)
library(pracma)
library(glmnet)
```

##Background
Polar-orbiting satellites have been carrying variety of radiometer instruments with infrared (IR) channels suitable for estimating sea surface temperature (SST) since 1981. The instruments designed for SST estimations have channels that are located at selected wavelengths where the atmosphere is relatively transparent. At these IR wavelengths the ocean surface emits radiation almost as a blackbody. In principle, without an absorbing and emitting atmosphere between the sea surface and the satellite, it would be possible to estimate SST using a single-channel measurement. In reality, surface-leaving IR radiance is partially attenuated by the atmosphere before it reaches a satellite sensor. Therefore it is necessary to make corrections for atmospheric effects [Minnett, 1990]. Several techniques have been proposed over the years to account for the atmospheric absorption of surface IR radiance to improve the accuracy of satellite retrievals of SST. Anding and Kauth [1970] found that the difference in measurement at two properly selected IR channels is proportional to the amount of atmospheric correction required. Using differences in brightness temperatures (BT) measured by an early satellite radiometer, [Prabhakra et. al. 1974] estimated SST to reasonable accuracy. Barton [1995] showed that this differential absorption between channels is exploited in all IR SST algorithms and demonstrated that there is a basic form to most algorithms.

##Load Data
```{r start}
In <- nc_open("/Users/changyaohua/Desktop/project\ 3/matchups_NPP_2018-01.nc")
In2 <- nc_open("/Users/changyaohua/Desktop/project\ 3/matchups_NPP_2018-02.nc")

#to see all the variable names in ncdf file:
names(In$var)

sst_reg <- ncvar_get(In, "sst_reg")
sst_insitu <- ncvar_get(In, "sst_insitu")
sst_ref <- ncvar_get(In, "sst_ref")
sza <- ncvar_get(In, 'sza')
vza <- ncvar_get(In, 'vza')

sec_theta = sec(vza) - 1

BT_M12 <- ncvar_get(In, "BT_M12")
BT_M13 <- ncvar_get(In, "BT_M13")
BT_M14 <- ncvar_get(In, "BT_M14")
BT_M15 <- ncvar_get(In, "BT_M15")
BT_M16 <- ncvar_get(In, "BT_M16")


dataframe1 = data.frame(sst_insitu, BT_M12, BT_M13, BT_M14, BT_M15, BT_M16)
head(dataframe1)

```

## Question 1
Use subset selection methods to find the best 2 variablesðœ†! in equation (1).  
Since there are only 2 variables, exhaustive search is an appropriate method.  
```{r}
# Question 1
# Use subset selection methods to find the best two-variable model using exhaustive search.
regfit.full = regsubsets(sst_insitu ~ ., data = dataframe1, nvmax=2)
reg.summary = summary(regfit.full)

reg.summary$rsq
reg.summary$which
```

**Conclusion:** The output indicates that the best two-variable model contains only BT_M15 and BT_M16.

```{r}
model1 = lm(sst_insitu ~ BT_M15 + I(BT_M16 - BT_M15), data = dataframe1)

model2 = lm(sst_insitu ~ BT_M16 + I(BT_M15 - BT_M16), data = dataframe1)

# To evaluate the performance of model1, the adjusted R squared is 0.979362
summary1 = summary(model1)
model1_adj_r_squared = summary1$adj.r.squared
summary1$coefficients
# the performance of equation(1)
model1_adj_r_squared
sqrt(mean(model1$residuals ^ 2))


# To evaluate the performance of model2, the adjusted R squared is 0.979362 as well
summary2 = summary(model2)
model2_adj_r_squared = summary2$adj.r.squared
summary2$coefficients
# the performance of equation(1)
model2_adj_r_squared
sqrt(mean(model2$residuals ^ 2))
```

# Question 2
Use a selection method to find the best coefficients in the equation (2).  
Compare the performance with (1) and with operational Ts (â€˜sst_regâ€™ of netcdf).  
Use independent data set for evaluation.   
```{r }
model3 = lm(sst_insitu ~ BT_M15 + I(BT_M15 - BT_M16):sst_ref + I(BT_M15 - BT_M16):sec_theta, data = dataframe1)
summary3 = summary(model3)
model3_adj_r_squared = summary3$adj.r.squared
summary3$coefficients
# the performance of equation(2)
model3_adj_r_squared
sqrt(mean(model3$residuals ^ 2))

# the performance of operational ð‘‡
sqrt(mean((sst_insitu - sst_reg) ^ 2))
```
Model | RMSE | R^2^  
------------- |------------ |--------------
equation (2) | 1.037475 | 0.9797013
equation (1) | 1.046112 | 0.979362
operational Ts | 0.3289296 | null

**Conclusion:** the performance of equation (2) is better than equation (1) a little bit, but it is worse than operational Ts.


## Question 3
```{r}
model4 = lm(sst_insitu ~ BT_M15 + I(BT_M15 - BT_M16):sst_ref + I(BT_M15 - BT_M16):sec_theta + sst_ref, data = dataframe1)
summary4 = summary(model4)
model4_adj_r_squared = summary4$adj.r.squared
summary4$coefficients
# the performance of equation(3)
model4_adj_r_squared
sqrt(mean(model4$residuals ^ 2))
```

Model | RMSE | R^2^  
------------- |------------ |--------------
equation (2a) | 0.2380083 | 0.9989317
equation (2) | 1.037475 | 0.9797013
equation (1) | 1.046112 | 0.979362
operational Ts | 0.3289296 | null

**Conclusion:** the performance of equation (2a) is better than equation (1) and equation (2) a lot, but it is worse than operational Ts a little bit.

# Question 4
Estimate the coefficients using shrinkage methods (Lasso and Ridge regression).   
Evaluate and compare with operational Ts.    
Use independent data set for evaluation. 

### Regression Equation night (3n)
```{r}
#Lasso and Ridge methods involve regularization of regression coefficients.

#In Ridge Regression, the ordinary least squares loss function is augmented in such a way that the sum of squared residuals is minimized #but also the size of parameter estimates are penalized in order to shrink them towards zero:

#In Lasso (Least Absolute Shrinkage and Selection Operator), similar to Ridge, also adds a penalty for non-zero coefficients, but unlike #ridge regression which penalizes sum of squared coefficients, Lasso penalizes the sum of absolute values. 

#As a result, for high values of Î», many coefficients are exactly zeroed under lasso, which is never the case in ridge regression.



dataframe2 = data.frame(sza,sec_theta,sst_insitu, BT_M12, BT_M13, BT_M14, BT_M15, BT_M16, sst_ref)
head(dataframe2,10)

night_time = dataframe2[(dataframe2$sza >= 95) & (dataframe2$sza <= 180), ]
head(night_time)


x_night=model.matrix(sst_insitu ~  BT_M15 + I(BT_M15 - BT_M12) + I(BT_M15 - BT_M14) + I(BT_M15 - BT_M16) 
               + sec_theta + BT_M15:sec_theta + I(BT_M15 - BT_M12):sec_theta 
               + I(BT_M15 - BT_M14):sec_theta 
               + I(BT_M15 - BT_M16):sec_theta + BT_M15:sst_ref + I(BT_M15 - BT_M12):sst_ref 
               + I(BT_M15 - BT_M14):sst_ref + I(BT_M15 - BT_M16):sst_ref, night_time)[,-1]
y_night=as.vector(night_time$sst_insitu)

set.seed (1)
train=sample(1:nrow(x_night), nrow(x_night)/2)
test=(-train)
y_night.test=y_night[test]

#set.seed (1)
#cv.out=cv.glmnet(x[train ,],y[train],alpha=0, nfolds=3)
#bestlam=cv.out$lambda.min
#bestlam

grid=10^seq(10,-2,length=100)

#Ridge regression for night (3n)
out=glmnet(x_night,y_night,alpha=0)
ridge.mod=glmnet(x_night,y_night,alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,s=min(out$lambda) ,newx=x_night[test,])
# Estimate the coefficients using ridge regression
predict(out,type="coefficients",s=min(out$lambda))[1:14,]
# The performance of equation(3n) using ridge regression
sqrt(mean((ridge.pred-y_night.test)^2))

# Lasso regression for night (3n)
out=glmnet(x_night,y_night,alpha=1)
lasso.mod=glmnet(x_night[train ,],y_night[train],alpha=1,lambda=grid)
lasso.pred=predict(lasso.mod,s=min(out$lambda) ,newx=x_night[test,])
# Estimate the coefficients using Lasso regression
predict(out,type="coefficients",s=min(out$lambda))[1:14,]
# The performance of equation(3n) using Lasso regression
sqrt(mean((lasso.pred-y_night.test)^2))
```

### Regression Equation day (3d)
```{r}
day_time = dataframe2[(dataframe2$sza >= 0) & (dataframe2$sza <= 85), ]
head(day_time)

x_day=model.matrix(sst_insitu ~ BT_M15 + I(BT_M15 - BT_M14) + I(BT_M15 - BT_M16) + sec_theta 
                   + BT_M15:sec_theta 
                   + I(BT_M15 - BT_M14):sec_theta + I(BT_M15 - BT_M16):sec_theta + BT_M15:sst_ref 
                   + I(BT_M15 - BT_M14):sst_ref + I(BT_M15 - BT_M16):sst_ref, day_time)[,-1]
y_day=as.vector(day_time$sst_insitu)

set.seed (1)
train=sample(1:nrow(x_day), nrow(x_day)/2)
test=(-train)
y_day.test=y_day[test]

grid=10^seq(10,-2,length=100)

# Ridge regression for day (3d) 
out=glmnet(x_day,y_day,alpha=0)
ridge.mod=glmnet(x_day,y_day,alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,s=min(out$lambda) ,newx=x_day[test,])
# Estimate the coefficients using ridge regression
predict(out,type="coefficients",s=min(out$lambda))[1:11,]
# The performance of equation(3d) using ridge regression
sqrt(mean((ridge.pred-y_day.test)^2))

#Lasso regression for day (3d) 
out=glmnet(x_day,y_day,alpha=1)
lasso.mod=glmnet(x_day[train ,],y_day[train],alpha=1,lambda=grid)
lasso.pred=predict(lasso.mod,s=min(out$lambda) ,newx=x_day[test,])
# Estimate the coefficients using Lasso regression
predict(out,type="coefficients",s=min(out$lambda))[1:11,]
# The performance of equation(3d) using Lasso regression
sqrt(mean((lasso.pred-y_day.test)^2))
```

Model | RMSE 
------------- |------------ 
equation (3n) using ridge regression | 0.4893441 
equation (3n) using lasso regression | 0.2767067 
equation (3d) using ridge regression | 0.6798317 
equation (3d) using lasso regression | 0.4430136 
operational Ts | 0.3289296 

**Conclusion:** only the performance of equation (3n) using lasso regression is better than operational Ts a little bit, while other equations are worse than operational Ts.

##Question 5
Starting from the form (3n) and (3d), use backward selection method to reduce number of terms without significant loss of performance.  
```{r }
regfit.bwd = regsubsets(sst_insitu ~ BT_M15 + I(BT_M15 - BT_M12) + I(BT_M15 - BT_M14)
                        + I(BT_M15 - BT_M16) + sec_theta + BT_M15:sec_theta 
                        + I(BT_M15 - BT_M12):sec_theta + I(BT_M15 - BT_M14):sec_theta 
                        + I(BT_M15 - BT_M16):sec_theta + BT_M15:sst_ref + I(BT_M15 - BT_M12):sst_ref 
                        + I(BT_M15 - BT_M14):sst_ref + I(BT_M15 - BT_M16):sst_ref, 
                        data = night_time, method='backward', nvmax=13)
regfit.summary = summary(regfit.bwd)
regfit.summary$which
regfit.summary$adjr2
regfit.summary$bic

coef(regfit.bwd, 2)
```

**Conclusion:** The result tells us that only keeping b1,b10 and remove other variables is without significant loss of performance

```{r}
regfit.bwd = regsubsets(sst_insitu ~ BT_M15 + I(BT_M15 - BT_M14) + I(BT_M15 - BT_M16) + sec_theta + BT_M15:sec_theta 
                        + I(BT_M15 - BT_M14):sec_theta + I(BT_M15 - BT_M16):sec_theta + BT_M15:sst_ref 
                        + I(BT_M15 - BT_M14):sst_ref + I(BT_M15 - BT_M16):sst_ref, 
                        data = day_time, method='backward', nvmax=10)
regfit.summary = summary(regfit.bwd)
regfit.summary$which
regfit.summary$adjr2
regfit.summary$bic
regfit.summary$cp
coef(regfit.bwd, 2)
```

**Conclusion:**The result tells us that only keeping b1,b10 and remove other variables is without significant loss of performance

``Compare with Lasso and Ridge results.  
`{r}
model6 = lm(sst_insitu ~ BT_M15 + BT_M15:sst_ref, data = night_time)
model6_r_squared = summary(model6)$adj.r.squared
model6_r_squared
sqrt(mean(model6$residuals^2))

model7 = lm(sst_insitu ~ BT_M15 + BT_M15:sst_ref, data = day_time)
model7_r_squared = summary(model6)$adj.r.squared
model7_r_squared
sqrt(mean(model7$residuals^2))
```
Codel | RMSE 
------------- |------------ 
equation (3n) using backward selection method | 0.2535694 
equation (3d) using backward selection method | 0.2807222 
equation (3n) using ridge regression | 0.4893441 
equation (3n) using lasso regression | 0.2767067 
equation (3d) using ridge regression | 0.6798317 
equation (3d) using lasso regression | 0.4430136 
operational Ts | 0.3289296 

**Conclusion:** Compare with Lasso and Ridge results, the performance of equation (3n) and equation (3d) improve after using backward selection method.

What is the best model obtained according Cp, AIC, BIC, and adjusted R2? 
```{r}
regfit.summary$adjr2
regfit.summary$bic
regfit.summary$cp

n <- length(sst_insitu)
p <- 10
aic = regfit.summary$bic + (2-log(n))*(p+1)

par(mfrow=c(2,2))
plot(regfit.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
max_adjr2 = which.max(regfit.summary$adjr2)
points(max_adjr2,regfit.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)
plot(regfit.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
min_cp = which.min(regfit.summary$cp )
points(min_cp,regfit.summary$cp[min_cp],col="red",cex=2,pch=20)
plot(regfit.summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
min_bic = which.min(regfit.summary$bic )
points(min_bic,regfit.summary$bic[min_bic],col="red",cex=2,pch=20)
plot(x = seq(1,p),y = aic, xlab="Number of Variables ", ylab = "AIC",type="l")
min_aic = which.min(aic)
points(min_aic,aic[min_aic],col="red",cex=2,pch=20)
```

**Conclusion:** These plots tell us that best model obtained according Cp and adjusted R2 when there are eight variables in the model, while best model obtained according AIC, and BIC when there are six variables in the model.

Show some plots to provide evidence for your answer.
```{r}
plot(regfit.bwd, scale = "Cp")
plot(regfit.bwd, scale = "adjr2")
plot(regfit.bwd, scale = "bic")

```

Report the coefficients of the best model obtained.
```{r}
coef(regfit.bwd, 8)
coef(regfit.bwd, 6)
```

##Question 6
Perform best subset selection on the training set (one of the matchup files), and plot the training set (using another matchup file) RMSE associated with the best model of each size.   
```{r}
regfit.full = regsubsets(sst_insitu ~ ., data = dataframe2, nvmax=8)
reg.summary = summary(regfit.full)

reg.summary$rsq
reg.summary$which

sqrt(mean(reg.summary$rss^2))

sst_insitu_2 <- ncvar_get(In2, "sst_insitu")
sst_ref_2 <- ncvar_get(In2, "sst_ref")
sza_2 <- ncvar_get(In2, 'sza')
vza_2 <- ncvar_get(In2, 'vza')

BT_M12_2 <- ncvar_get(In2, "BT_M12")
BT_M13_2 <- ncvar_get(In2, "BT_M13")
BT_M14_2 <- ncvar_get(In2, "BT_M14")
BT_M15_2 <- ncvar_get(In2, "BT_M15")
BT_M16_2 <- ncvar_get(In2, "BT_M16")


dataframe3 = data.frame(sst_insitu_2, BT_M12_2, BT_M13_2, BT_M14_2, BT_M15_2, BT_M16_2, sst_ref_2, sza_2, vza_2)
head(dataframe3)

model_varibale_1 = lm(sst_insitu_2~sst_ref_2, data=dataframe3)
rmse_varibale_1 = sqrt(mean(model_varibale_1$residuals^2))

model_varibale_2 = lm(sst_insitu_2~sst_ref_2 + sza_2, data=dataframe3)
rmse_varibale_2 = sqrt(mean(model_varibale_2$residuals^2))

model_varibale_3 = lm(sst_insitu_2~sst_ref_2 + sza_2 + BT_M15_2, data=dataframe3)
rmse_varibale_3 = sqrt(mean(model_varibale_3$residuals^2))

model_varibale_4 = lm(sst_insitu_2~sst_ref_2 + sza_2 + BT_M15_2 + BT_M16_2, data=dataframe3)
rmse_varibale_4 = sqrt(mean(model_varibale_4$residuals^2))

#model_varibale_5 = lm(sst_insitu_2~sst_ref_2 + sza_2 + BT_M15_2 + BT_M16_2 + BT_M13_2, data=dataframe3)
#rmse_varibale_5 = sqrt(mean(model_varibale_5$residuals^2))

#model_varibale_6 = lm(sst_insitu_2~sst_ref_2 + sza_2 + BT_M15_2 + BT_M16_2 + BT_M13_2 + BT_M12_2, data=dataframe3)
#rmse_varibale_6 = sqrt(mean(model_varibale_6$residuals^2))

#model_varibale_7 = lm(sst_insitu_2~sst_ref_2 + sza_2 + BT_M15_2 + BT_M16_2 + BT_M13_2 + BT_M12_2 + BT_M14_2, data=dataframe3)
#rmse_varibale_7 = sqrt(mean(model_varibale_7$residuals^2))

#model_varibale_8 = lm(sst_insitu_2~sst_ref_2 + sza_2 + BT_M15_2 + BT_M16_2 + BT_M13_2 + BT_M12_2 + BT_M14_2 + sec_theta, data=dataframe3)
#rmse_varibale_8 = sqrt(mean(model_varibale_8$residuals^2))


plot(c(1,2,3,4), c(rmse_varibale_1, rmse_varibale_2, rmse_varibale_3, rmse_varibale_4), type='b', xlab = 'Size of best model', ylab = 'RMSE')

```

For which model does the test set MSE take on its minimum value?   

How does the model at which the test set MSE is minimized compare to the model used to generate the data?  
Does this agree with the coefficient values of the model?


**Conclusion:**

## Question 7
Estimate the test error of your best model using the validation set approach (k- fold and/or bootstrap).   
Does it agree with your findings in part 6? Comment on the result.

**Conclusion:**

## References
Anding, D. and R. Kauth. 1970. Estimation of sea surface temperature from space, Remote Sens. Environ., 1, 217-220.

Barton, I.J., 1995. Satellite-derived sea surface temperatures: Current status. Journal of Geophysical Research 100: 8777â€“8790.

Minnett, P.J. 1990. The regional optimization of infrared measurements of sea surface temperature from space. J. Geophys Res., 95

Petrenko, B.; Ignatov, A.; Kihai, Y.; Stroup, J.; Dash, P. 2014. Evaluation and selection of SST regression algorithms for JPSS VIIRS. J. Geophys. Res., 119, 4580â€“4599 https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2013JD020637

Prabhakara, C., G. Dalau, and V.G. Kunde. 1974. Estimation of sea surface temperature from remote sensing in the 11 to 13mm window region, J. Geophys. Res., 79, 5039-5044.
              
Walton, C. C., W. G. Pichel, F. J. Sapper, and D. A. May, 1988. The development and operational application of nonlinear algorithms for the measurement of sea surface temperatures with NOAA polar-orbiting environmental satellites. Journal of Geophysical Research 103: 27999â€“28012.
